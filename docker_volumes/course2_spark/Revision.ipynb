{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Certified Associate Developer for Apache Spark 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/\n",
    "\n",
    "\n",
    "Content: \n",
    "* Spark Architecture (30%)\n",
    "* SQL - DF (40%) https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "* RDD low API (10%)\n",
    "* Streaming (10%) https://spark.apache.org/docs/latest/streaming-programming-guide.html_\n",
    "* ML (5%) https://spark.apache.org/docs/latest/ml-guide.html\n",
    "* graphX (5%) https://spark.apache.org/docs/latest/graphx-programming-guide.html\n",
    "\n",
    "Courses from https://academy.databricks.com/instructor-led-training/apache-spark-programming\n",
    "* Day 1: DataFrames\n",
    " * __1 Introduction__ Course overview, Databricks ecosystem, Spark overview, Case study, Knowledge check\n",
    " * __2 Databricks Platform__ Databricks concepts, Workspace UI, Notebooks, Lab\n",
    " * __3 Spark SQL__ Spark SQL module, Documentation, DataFrame concepts, Lab\n",
    " * __4 Reader & Writer__ Data Sources, DataFrameReader & Writer, Schemas, Performance, Lab\n",
    " * __5 DataFrame & Column__ Columns and expressions, Transformations, Actions, Rows, Lab\n",
    " \n",
    "* Day 2: Transformations\n",
    " * __1 Aggregation__ Groupby, Grouped data methods, Aggregate functions, Math functions, Lab\n",
    " * __2 Datetimes__ Dates & Timestamps, Datetime patterns, Datetime functions, Lab\n",
    " * __3 Complex Types__ String functions, Collection functions\n",
    " * __4 Additional Function__s Non-aggregate functions, NaFunctions, Lab\n",
    " * __5 User-Defined Functions__ User-defined functions, Vectorized UDFs, Performance, Lab\n",
    " \n",
    "* Day 3: Spark Optimization\n",
    " * __1 Spark Architecture__ Spark Cluster, Spark Execution, Shuffling, Lab\n",
    " * __2 Shuffles & Caching__ Lineage, Shuffle files, Caching, Caching recommendations, Spark UI: Storage, Lab\n",
    " * __3 Query Optimization__ Catalyst Optimizer, Adaptive Query Execution, Best practices, Lab\n",
    " * __4 Spark UI__ Spark UI navigation, Spark UI: Jobs, Stages, SQL\n",
    " * __5 Partitioning__ Partitions vs cores, Default shuffle partitions, Repartition, Best practices, AQE, Lab\n",
    " \n",
    "* Day 4: Structured Streaming\n",
    " * __1 Review__ DataFrames and Transformations, Lab\n",
    " * __2 Streaming Query__ Streaming concepts, Sources and Sinks, Streaming Query, Transformations, Lab\n",
    " * __3 Processing Streams__ Monitoring Streams, Lab\n",
    " * __4 Aggregating Streams__ Streaming aggregations, Windows, Watermarking, Lab\n",
    " * __5 Delta Lake__ Delta Lake concepts, Batch and streaming, Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "videos :\n",
    "\n",
    "https://www.youtube.com/watch?v=7ooZ4S7Ay6Y\n",
    "\n",
    "https://www.youtube.com/watch?v=tFRPeU5HemU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# I) Spark Architecture (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) SQL Df (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) RDD low API (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Streaming (10%)\n",
    "Most of examples are taken from \\\n",
    "_https://spark.apache.org/docs/latest/streaming-programming-guide.html_ \\\n",
    "I tried them and completed them to be able to execute them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\"\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import StreamingContext, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines DStream represents the stream of data that will be received from the data server. Each record in this DStream is a line of text. Next, we want to split the lines by space into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. In this case, each line will be split into multiple words and the stream of words is represented as the words DStream. Next, we want to count these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words DStream is further mapped (one-to-one transformation) to a DStream of (word, 1) pairs, which is then reduced to get the frequency of words in each batch of data. Finally, wordCounts.pprint() will print a few of the counts generated every second.\n",
    "\n",
    "Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet. To start the processing after all the transformations have been setup, we finally call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-11-12 11:13:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-12 11:13:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-12 11:13:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-12 11:13:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "#ssc.awaitTermination()  # Wait for the computation to terminate due to manual exit or error\n",
    "import time\n",
    "time.sleep(3)\n",
    "ssc.stop(stopSparkContext=False)  # Stop listening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an other terminal run the command to listen the port\n",
    "\n",
    "nc -l 10222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After a context is defined, you have to do the following.\n",
    "\n",
    "    Define the input sources by creating input DStreams.\n",
    "    Define the streaming computations by applying transformation and output operations to DStreams.\n",
    "    Start receiving data and processing it using streamingContext.start().\n",
    "    Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().\n",
    "    The processing can be manually stopped using streamingContext.stop().\n",
    "\n",
    "Points to remember:\n",
    "\n",
    "    Once a context has been started, no new streaming computations can be set up or added to it.\n",
    "    Once a context has been stopped, it cannot be restarted.\n",
    "    Only one StreamingContext can be active in a JVM at the same time.\n",
    "    stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.\n",
    "    A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.\"\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretized stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stream is defined as a serie of discrete RDDs inscreasing by time and where all the transformation are applied \\\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_image from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input DStreams and Receivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" Input DStreams are DStreams representing the stream of input data received from streaming sources. In the quick example, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.\n",
    "\n",
    "Spark Streaming provides two categories of built-in streaming sources.\n",
    "\n",
    "    Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.\n",
    "    Advanced sources: Sources like Kafka, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.\n",
    "\n",
    "We are going to discuss some of the sources present in each category later in this section.\n",
    "\n",
    "Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the Performance Tuning section). This will create multiple receivers which will simultaneously receive multiple data streams. But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received data, as well as to run the receiver(s).\n",
    "\n",
    "Points to remember\n",
    "\n",
    "    When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).\n",
    "\n",
    "    Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it. \"\n",
    "\n",
    "_by https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get files like text log, when they arrive in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Utilisateurs\\\\Millet\\\\AppData\\\\Roaming\\\\IBM Watson Studio\\\\logs\\\\\"\n",
    "#ssc = StreamingContext.textFileStream(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files must be in the same data format.\n",
    "\n",
    "A file is considered part of a time period based on its modification time, not its creation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue of RDD for test purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queueOfRDDs = [sc.range(0, 1000),\n",
    "               sc.range(1000, 3000),\n",
    "               sc.range(2000, 4000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_received = ssc.queueStream(queueOfRDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_received.count().pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:24\n",
      "-------------------------------------------\n",
      "1000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:25\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:26\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:27\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:28\n",
      "-------------------------------------------\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "#ssc.awaitTermination()\n",
    "\n",
    "import time\n",
    "\n",
    "# Note Changes to the queue after the stream is created will not be recognized. \n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html?highlight=queue\n",
    "queueOfRDDs.append(sc.range(0, 100))\n",
    "\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 kinds of receivers\n",
    "\n",
    "Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication. (kafka...)\n",
    "\n",
    "Unreliable Receiver - An unreliable receiver does not send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations on DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Transformation\tMeaning\n",
    "* __map(func)__ \t    Return a new DStream by passing each element of the source DStream through a function func.\n",
    "* __flatMap(func)__ \tSimilar to map, but each input item can be mapped to 0 or more output items.\n",
    "* __filter(func)__ \tReturn a new DStream by selecting only the records of the source DStream on which func returns true.\n",
    "* __repartition(numPartitions)__ \tChanges the level of parallelism in this DStream by creating more or fewer partitions.\n",
    "* __union(otherStream)__ \tReturn a new DStream that contains the union of the elements in the source DStream and otherDStream.\n",
    "* __count()__ \tReturn a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.\n",
    "* __reduce(func)__ \tReturn a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "* __countByValue()__ \tWhen called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "* __reduceByKey(func, [numTasks])__ \tWhen called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "* __join(otherStream, [numTasks])__ \tWhen called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "* __cogroup(otherStream, [numTasks])__ \tWhen called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "* __transform(func)__ \tReturn a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary * RDD operations on the DStream.\n",
    "* __updateStateByKey(func)__ \tReturn a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.\" \\\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical transformations example\n",
    "Dummy example: Get the number of letters for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:29\n",
      "-------------------------------------------\n",
      "21\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:30\n",
      "-------------------------------------------\n",
      "27\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:31\n",
      "-------------------------------------------\n",
      "44\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of map/reduce/count on StreamingContext\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "                     \n",
    "# Map the words with their lengths - cache because lineLengths is called with 2 actions\n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : len(word))\n",
    "# Count the words\n",
    "total_length = line_lengths.reduce(lambda a, b: a + b)\n",
    "# Display result\n",
    "total_length.pprint()\n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updateStateByKey transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: get the number of occurence of each word in a stream. not per document. \\\n",
    "Only one result is sent, and is updated for each new rdd coming. \\\n",
    "After the last RDD, the result is provided each second. \\\n",
    "Checkpoint use is necessaty here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:35\n",
      "-------------------------------------------\n",
      "('is', 1)\n",
      "('This', 1)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "('document', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:36\n",
      "-------------------------------------------\n",
      "('is', 2)\n",
      "('longer', 1)\n",
      "('This', 2)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "('document', 2)\n",
      "('a', 1)\n",
      "('second', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:37\n",
      "-------------------------------------------\n",
      "('is', 4)\n",
      "('longer', 1)\n",
      "(\"doesn't\", 1)\n",
      "('know', 1)\n",
      "('This', 3)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "('document', 4)\n",
      "('a', 3)\n",
      "('second', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:38\n",
      "-------------------------------------------\n",
      "('is', 4)\n",
      "('longer', 1)\n",
      "(\"doesn't\", 1)\n",
      "('know', 1)\n",
      "('This', 3)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "('document', 4)\n",
      "('a', 3)\n",
      "('second', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:39\n",
      "-------------------------------------------\n",
      "('is', 4)\n",
      "('longer', 1)\n",
      "(\"doesn't\", 1)\n",
      "('know', 1)\n",
      "('This', 3)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "('document', 4)\n",
      "('a', 3)\n",
      "('second', 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of updateStateByKey use\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "def update_word_count(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount) \n",
    "\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint_directory\")\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "                    \n",
    "# Map the words with their lengths - cache because lineLengths is called with 2 actions\n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : (word, 1))\n",
    "\n",
    "# Count the words\n",
    "word_count = line_lengths.reduceByKey(lambda a, b : a + b)\n",
    "# word_count.pprint() # If the occurence of words per document is needed\n",
    "\n",
    "# update each rdd state  by key when other rdd comes\n",
    "total_word_count = word_count.updateStateByKey(update_word_count)\n",
    "\n",
    "# Display result\n",
    "total_word_count.pprint()\n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform(func) operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The transform operation (along with its variations like transformWith) allows arbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD operation that is not exposed in the DStream API. For example, the functionality of joining every batch in a data stream with another dataset is not directly exposed in the DStream API. However, you can easily use transform to do this. This enables very powerful possibilities. For example, one can do real-time data cleaning by joining the input data stream with precomputed spam information (maybe generated with Spark as well) and then filtering based on it.\" \n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Get the number of word occurence in the whole stream, \\\n",
    "with a transformation to filter unwanted words \\\n",
    "The rdd with unwanted words will be joined and filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('first', (1, None)),\n",
       " ('a', (None, 1)),\n",
       " ('is', (1, 1)),\n",
       " ('This', (1, 1)),\n",
       " ('document', (1, None)),\n",
       " ('document', (1, None)),\n",
       " ('my', (1, 1))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('first', (1, None)), ('document', (1, None)), ('document', (1, None))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('first', 1), ('document', 1), ('document', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('first', 1), ('document', 2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With a normal context outside a stream, the code would have been\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "# Initilize document and filter\n",
    "rdd_document = sc.parallelize([\"This document is my first document\"])\n",
    "rdd_unwanted_words = sc.parallelize([\"This\", \"is\", \"a\", \"my\"])\n",
    "\n",
    "\n",
    "# Map the words \n",
    "words = rdd_document.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# format the RDD with key value pairs\n",
    "lines = words.map(lambda word : (word, 1))\n",
    "unwanted = rdd_unwanted_words.map(lambda word : (word, 1))\n",
    "\n",
    "#  Outer Join on the 2 rdds \n",
    "doc_clnd = lines.fullOuterJoin(unwanted)\n",
    "display(doc_clnd.collect())\n",
    "doc_clnd2 = doc_clnd.filter(lambda x: x[1][0] == 1 and  x[1][1] is None)\n",
    "display(doc_clnd2.collect())\n",
    "doc_clnd3 = doc_clnd2.map(lambda x: (x[0], 1))\n",
    "display(doc_clnd3.collect())\n",
    "\n",
    "# reduce to get the count of word occurence\n",
    "word_count = doc_clnd3.reduceByKey(lambda a, b : a + b)\n",
    "word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:41\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('document', 1)\n",
      "('document', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:41\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('document', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:42\n",
      "-------------------------------------------\n",
      "('longer', 1)\n",
      "('second', 1)\n",
      "('document', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:42\n",
      "-------------------------------------------\n",
      "('longer', 1)\n",
      "('second', 1)\n",
      "('document', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:43\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('which', 1)\n",
      "('know', 1)\n",
      "('document', 1)\n",
      "('document', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:43\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('know', 1)\n",
      "('which', 1)\n",
      "('document', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With transfert in a stream\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "# RDD to filter\n",
    "rdd_unwanted_words = sc.parallelize([\"This\", \"is\", \"a\", \"my\", \"it\"])\n",
    "\n",
    "# Map the words \n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "unwanted_words = rdd_unwanted_words.map(lambda word : (word, 1))\n",
    "\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : (word, 1))\n",
    "\n",
    "#line_lengths.pprint()\n",
    "cleanedDStream = line_lengths.transform(lambda rdd: rdd.fullOuterJoin(unwanted_words) \\\n",
    "                                                       .filter(lambda x: x[1][0] == 1 and  x[1][1] is None) \\\n",
    "                                                       .map(lambda x: (x[0], 1)))\n",
    "cleanedDStream.pprint()\n",
    "\n",
    "# Count the words\n",
    "word_count = cleanedDStream.reduceByKey(lambda a, b : a + b)\n",
    "word_count.pprint() \n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Spark Streaming also provides windowed computations, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window.\"\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-window.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "     \n",
    "\"Some of the common window operations are as follows. All of these operations take the said two parameters - windowLength and slideInterval.\n",
    "* __window(windowLength, slideInterval)__ \tReturn a new DStream which is computed based on windowed batches of the source DStream.\n",
    "* __countByWindow(windowLength, slideInterval__) \tReturn a sliding window count of elements in the stream.\n",
    "* __reduceByWindow(func, windowLength, slideInterval)__ \tReturn a new single-element stream, created by aggregating elements in the stream over a sliding interval using func. The function should be associative and commutative so that it can be computed correctly in parallel.\n",
    "* __reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])__ \tWhen called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "* __reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])__ A more efficient version of the above reduceByKeyAndWindow() where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter invFunc). Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. Note that checkpointing must be enabled for using this operation.\n",
    "* __countByValueAndWindow(windowLength, slideInterval, [numTasks])__ \tWhen called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. \n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Count the number of occurence of each words in the document \\\n",
    "This time does it reducing the last 2 documents in one rdd \\\n",
    "Only the first and the last (n+1) return a result from an unique rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:47\n",
      "-------------------------------------------\n",
      "('is', 1)\n",
      "('This', 1)\n",
      "('document', 2)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:48\n",
      "-------------------------------------------\n",
      "('is', 2)\n",
      "('longer', 1)\n",
      "('This', 2)\n",
      "('document', 3)\n",
      "('my', 1)\n",
      "('first', 1)\n",
      "('a', 1)\n",
      "('second', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:49\n",
      "-------------------------------------------\n",
      "('is', 3)\n",
      "('longer', 1)\n",
      "(\"doesn't\", 1)\n",
      "('know', 1)\n",
      "('This', 2)\n",
      "('a', 3)\n",
      "('second', 1)\n",
      "('document', 3)\n",
      "('which', 1)\n",
      "('it', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:50\n",
      "-------------------------------------------\n",
      "('is', 2)\n",
      "(\"doesn't\", 1)\n",
      "('know', 1)\n",
      "('This', 1)\n",
      "('a', 2)\n",
      "('document', 2)\n",
      "('which', 1)\n",
      "('it', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:27:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example with reduceByKeyAndWindow in a stream\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "# Map the words \n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : (word, 1))\n",
    "# Count the words by window, reduce last 2 second every second\n",
    "word_count = line_lengths.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 2, 1)\n",
    "word_count.pprint() \n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Stream operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  join all the rdds of the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stream1 = ... \\\n",
    "stream2 = ... \\\n",
    "joinedStream = stream1.join(stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  join the rdds of specific windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windowed_stream1 = stream1.window(20) \\\n",
    "windowed_stream2 = stream2.window(60) \\\n",
    "joined_stream = windowed_stream1.join(windowed_stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform: stream and rdd join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = ... # some RDD \\\n",
    "windowed_stream = stream.window(20) \\\n",
    "joinedStream = windowed_stream.transform(lambda rdd: rdd.join(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Operations on DStreams\n",
    "\n",
    "* __print()__ \tPrints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.\n",
    "Python API This is called pprint() in the Python API.\n",
    "* __saveAsTextFiles(prefix, [suffix])__ \tSave this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "* __saveAsObjectFiles(prefix, [suffix])__ \tSave this DStream's contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "Python API This is not available in the Python API.\n",
    "* __saveAsHadoopFiles(prefix, [suffix])__ \tSave this DStream's contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "Python API This is not available in the Python API.\n",
    "* __foreachRDD(func)__ \tThe most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Patterns for using foreachRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a function to apply to each RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "def sendPartition(iter):\n",
    "    connection = createNewConnection()\n",
    "    for record in iter:\n",
    "        connection.send(record)\n",
    "    connection.close()\n",
    "\n",
    "# Initialize spark context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"])]\n",
    "\n",
    "# Initilize Streamcontext and execute te function for each rdd\n",
    "ssc = StreamingContext(sc, 1)\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "rdd_received.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "\n",
    "# Launch, wait and stop\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame and SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "========= 2020-11-03 23:27:57 =========\n",
      "========= 2020-11-03 23:27:58 =========\n",
      "========= 2020-11-03 23:27:59 =========\n",
      "========= 2020-11-03 23:28:00 =========\n",
      "========= 2020-11-03 23:28:01 =========\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    # here you can do things about RDD or SQL...\n",
    "    rdd_count = rdd.map(lambda x: len(x))\n",
    "    rdd_count.show()\n",
    "    \n",
    "\n",
    "# Initialize spark context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"])]\n",
    "\n",
    "# Initilize Streamcontext and execute te function for each rdd\n",
    "ssc = StreamingContext(sc, 1)\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "rdd_received.foreachRDD(process)\n",
    "\n",
    "# Launch, wait and stop\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't do anything because the queueStream is manual and the function awaits new data coming..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching / Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RDDs, DStreams also allow developers to persist the stream’s data in memory. That is, using the persist() method on a DStream will automatically persist every RDD of that DStream in memory. This is useful if the data in the DStream will be computed multiple times (e.g., multiple operations on the same data). For window-based operations like reduceByWindow and reduceByKeyAndWindow and state-based operations like updateStateByKey, this is implicitly true. Hence, DStreams generated by window-based operations are automatically persisted in memory, without the developer calling persist().\n",
    "\n",
    "For input streams that receive data over the network (such as, Kafka, sockets, etc.), the default persistence level is set to replicate the data to two nodes for fault-tolerance.\n",
    "\n",
    "Note that, unlike RDDs, the default persistence level of DStreams keeps the data serialized in memory. This is further discussed in the Performance Tuning section. More information on different persistence levels can be found in the Spark Programming Guide.\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "\"A streaming application must operate 24/7 and hence must be resilient to failures unrelated to the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible, Spark Streaming needs to checkpoint enough information to a fault- tolerant storage system such that it can recover from failures. There are two types of data that are checkpointed.\n",
    "\n",
    "* Metadata checkpointing - Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later). Metadata includes:\n",
    " * Configuration - The configuration that was used to create the streaming application.\n",
    " * DStream operations - The set of DStream operations that define the streaming application.\n",
    " * Incomplete batches - Batches whose jobs are queued but have not completed yet.\n",
    "\n",
    "* Data checkpointing - Saving of the generated RDDs to reliable storage. This is necessary in some stateful transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically checkpointed to reliable storage (e.g. HDFS) to cut off the dependency chains.\n",
    "\n",
    "To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used.\"\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:28:02\n",
      "-------------------------------------------\n",
      "This document is my first document\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:28:03\n",
      "-------------------------------------------\n",
      "This is a second longer document\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:28:04\n",
      "-------------------------------------------\n",
      "This is a document which doesn't know it is a document\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:28:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-03 23:28:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of use of checkpoints\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# set checkpoint directory\n",
    "ssc.checkpoint(\"checkpointDirectory\")  \n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "# set checkpoint Interval, 3 second\n",
    "rdd_received.checkpoint(3)\n",
    "\n",
    "# Execute an action\n",
    "rdd_received.pprint()\n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) ML (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for the part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark context\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# delete files from previous runs\n",
    "!rm -f hmp.parquet*\n",
    "\n",
    "# download the file containing the data in PARQUET format\n",
    "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
    "    \n",
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) ML fit, transform, pipelines, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fit, transform on StringIndexer estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+\n",
      "|  x|  y|  z|              source|      class|classIndex|\n",
      "+---+---+---+--------------------+-----------+----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "+---+---+---+--------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer#, VectorAssembler, Normalizer, OneHotEncoderEstimator\n",
    "est_indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "model = est_indexer.fit(df)\n",
    "df_indexed = model.transform(df)\n",
    "df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a second fit transform on OneHotEncoder estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "est_indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "est_encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "df_indexed = est_indexer.fit(df).transform(df)\n",
    "df_encoded = est_encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pipeline to make different transformation in a row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define transformations\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
    "model = pipeline.fit(df)\n",
    "df_result = model.transform(df)\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|            features|\n",
      "+------+--------------------+\n",
      "|   6.0|[0.20754716981132...|\n",
      "|   6.0|[0.20754716981132...|\n",
      "|   6.0|[0.20183486238532...|\n",
      "|   6.0|[0.20183486238532...|\n",
      "|   6.0|[0.19626168224299...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean dataframe for the next part\n",
    "df_cleaned = df_result.drop(\"x\").drop(\"y\").drop(\"z\").drop(\"source\").drop(\"class\").drop(\"categoryVec\").drop(\"features\")\n",
    "df_cleaned = df_cleaned.withColumnRenamed(\"classIndex\", \"target\")\n",
    "df_cleaned = df_cleaned.withColumnRenamed(\"features_norm\", \"features\")\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Model Selection, evaluator, parameter grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test data.\n",
    "df_learn, df_validation = df_cleaned.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and estimate model with fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "estim_lr = LinearRegression(featuresCol = 'features', labelCol='target',\n",
    "                            maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model_lr = estim_lr.fit(df_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [9.276474699807274,0.0,-2.7557906328884165]\n",
      "Intercept: 3.2530328439765346\n",
      "RMSE: 3.509002\n",
      "r2: 0.086105\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(model_lr.coefficients))\n",
    "print(\"Intercept: %s\" % str(model_lr.intercept))\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = model_lr.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+\n",
      "|target|            features|        prediction|\n",
      "+------+--------------------+------------------+\n",
      "|   0.0|[0.0,0.4852941176...|1.8346111946957322|\n",
      "|   0.0|[0.0,0.4933333333...|1.8567655899797368|\n",
      "|   0.0|[0.0,0.4941176470...|1.8589269943976887|\n",
      "|   0.0|       [0.0,0.5,0.5]|1.8751375275323263|\n",
      "|   0.0|       [0.0,0.5,0.5]|1.8751375275323263|\n",
      "+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.0862592\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "df_prediction = model_lr.transform(df_validation)\n",
    "df_prediction.show(5)\n",
    "\n",
    "# Evaluate model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval_lr = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                              labelCol=\"target\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % eval_lr.evaluate(df_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.51257\n"
     ]
    }
   ],
   "source": [
    "# The prediction and evaluation can be done in one step\n",
    "test_result = model_lr.evaluate(df_validation)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.0862592\n"
     ]
    }
   ],
   "source": [
    "# To summarize:\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train model\n",
    "estim_lr = LinearRegression(featuresCol = 'features', labelCol='target',\n",
    "                            maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model_lr = estim_lr.fit(df_learn)\n",
    "# Evaluate model\n",
    "df_prediction = model_lr.transform(df_validation)\n",
    "eval_lr = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                              labelCol=\"target\", metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % eval_lr.evaluate(df_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation selection to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,0.232,-0.017...|    0|\n",
      "|[0.0,0.111,-0.003...|    0|\n",
      "|[0.0,-0.391,-0.00...|    0|\n",
      "|[0.0,0.098,-0.012...|    0|\n",
      "|[0.0,0.026,-0.004...|    0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "temp_df = spark.createDataFrame([Row(V4366=0.0, V4460=0.232, V4916=-0.017, V1495=-0.104, V1639=0.005, V1967=-0.008, V3049=0.177, V3746=-0.675, V3869=-3.451, V524=0.004, V5409=0), Row(V4366=0.0, V4460=0.111, V4916=-0.003, V1495=-0.137, V1639=0.001, V1967=-0.01, V3049=0.01, V3746=-0.867, V3869=-2.759, V524=0.0, V5409=0), Row(V4366=0.0, V4460=-0.391, V4916=-0.003, V1495=-0.155, V1639=-0.006, V1967=-0.019, V3049=-0.706, V3746=0.166, V3869=0.189, V524=0.001, V5409=0), Row(V4366=0.0, V4460=0.098, V4916=-0.012, V1495=-0.108, V1639=0.005, V1967=-0.002, V3049=0.033, V3746=-0.787, V3869=-0.926, V524=0.002, V5409=0), Row(V4366=0.0, V4460=0.026, V4916=-0.004, V1495=-0.139, V1639=0.003, V1967=-0.006, V3049=-0.045, V3746=-0.208, V3869=-0.782, V524=0.001, V5409=0)])\n",
    "trainingData=temp_df.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\n",
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize estimators, pipeline and evaluators\n",
    "estim_lr = LinearRegression(featuresCol = 'features', labelCol='target', \n",
    "                            maxIter=10) #, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[estim_lr])\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                labelCol=\"target\", metricName=\"r2\")\n",
    "\n",
    "# Define the different parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estim_lr.regParam, [0.3, 0.5]) \\\n",
    "    .addGrid(estim_lr.elasticNetParam, [0.5, 0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Run cross validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "cvModel = crossval.fit(df_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6854132664170263\n",
      "[9.779454982679272,0.0,-4.111892840696731]\n",
      "0.3\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# print regression result and the best parameters found\n",
    "print(cvModel.bestModel.stages[0].intercept)\n",
    "print(cvModel.bestModel.stages[0].coefficients)\n",
    "print(cvModel.bestModel.stages[0].getRegParam())\n",
    "print(cvModel.bestModel.stages[0].getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.0862592\n"
     ]
    }
   ],
   "source": [
    "# CV model on the k folds, then train on the whole training set. Can be directly used for validation set\n",
    "df_prediction = cvModel.transform(df_validation)\n",
    "eval_lr = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                              labelCol=\"target\", \n",
    "                              metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % eval_lr.evaluate(df_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Graph (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
